#!/usr/bin/env python3
"""
Python MCP + LLM Implementation
Roo-Codeì˜ MCP êµ¬í˜„ì„ Pythonìœ¼ë¡œ ë³€í™˜í•œ ì˜ˆì œ

ì£¼ìš” ê¸°ëŠ¥:
1. MCP ì„œë²„ ì—°ê²° ë° ê´€ë¦¬
2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— MCP ì •ë³´ ë™ì  í¬í•¨
3. HTTP POSTë¡œ LLM API í˜¸ì¶œ
4. MCP ë„êµ¬ ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬
"""

import asyncio
import json
import subprocess
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from enum import Enum
import httpx
import aiofiles
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# ============================================================================
# 1. ë°ì´í„° ëª¨ë¸ ì •ì˜
# ============================================================================

class McpServerType(Enum):
    STDIO = "stdio"
    SSE = "sse"
    HTTP = "http"

@dataclass
class McpServerConfig:
    name: str
    type: McpServerType
    command: Optional[str] = None
    args: Optional[List[str]] = None
    env: Optional[Dict[str, str]] = None
    url: Optional[str] = None
    headers: Optional[Dict[str, str]] = None
    timeout: int = 60
    always_allow: List[str] = None
    disabled_tools: List[str] = None

    def __post_init__(self):
        if self.always_allow is None:
            self.always_allow = []
        if self.disabled_tools is None:
            self.disabled_tools = []

@dataclass
class McpTool:
    name: str
    description: str
    input_schema: Dict[str, Any]
    server_name: str

@dataclass
class McpResource:
    uri: str
    name: str
    description: str
    mime_type: Optional[str] = None

@dataclass
class LLMMessage:
    role: str  # "user", "assistant", "system"
    content: str

@dataclass
class LLMConfig:
    provider: str  # "openai", "anthropic", "ollama"
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    temperature: float = 0.7
    max_tokens: int = 4000

# ============================================================================
# 2. MCP Hub - MCP ì„œë²„ ê´€ë¦¬ í´ë˜ìŠ¤
# ============================================================================

class McpHub:
    """MCP ì„œë²„ë“¤ì„ ì—°ê²°í•˜ê³  ê´€ë¦¬í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.connections: Dict[str, ClientSession] = {}
        self.servers: Dict[str, McpServerConfig] = {}
        self.tools: Dict[str, List[McpTool]] = {}
        self.resources: Dict[str, List[McpResource]] = {}
        
    async def connect_server(self, config: McpServerConfig) -> bool:
        """MCP ì„œë²„ì— ì—°ê²°"""
        try:
            if config.type == McpServerType.STDIO:
                print(f"ğŸ”— MCP ì„œë²„ '{config.name}' ì—°ê²° ì‹œë„ ì¤‘...")
                
                # Stdio ê¸°ë°˜ MCP ì„œë²„ ì—°ê²°
                server_params = StdioServerParameters(
                    command=config.command,
                    args=config.args or [],
                    env=config.env or {}
                )
                
                # ê²€ì¦ëœ ìˆ˜ë™ MCP ì—°ê²° ë°©ì‹ ì‚¬ìš©
                try:
                    # 1. ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
                    import subprocess
                    process = subprocess.Popen(
                        [config.command] + (config.args or []),
                        stdin=subprocess.PIPE,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        text=True,
                        encoding='utf-8',  # UTF-8 ì¸ì½”ë”© ëª…ì‹œì  ì„¤ì •
                        bufsize=0
                    )
                    
                    # 2. ì´ˆê¸°í™” ìš”ì²­
                    init_request = {
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "initialize",
                        "params": {
                            "protocolVersion": "2024-11-05",
                            "capabilities": {
                                "roots": {"listChanged": True},
                                "sampling": {}
                            },
                            "clientInfo": {
                                "name": "python-mcp-llm-client",
                                "version": "1.0.0"
                            }
                        }
                    }
                    
                    # ìš”ì²­ ì „ì†¡
                    process.stdin.write(json.dumps(init_request) + "\n")
                    process.stdin.flush()
                    
                    # ì‘ë‹µ ëŒ€ê¸°
                    response_line = await asyncio.wait_for(
                        asyncio.to_thread(process.stdout.readline), 
                        timeout=10
                    )
                    response = json.loads(response_line.strip())
                    
                    if "error" in response:
                        raise Exception(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {response['error']}")
                    
                    # 3. initialized ì•Œë¦¼ ì „ì†¡
                    init_notification = {
                        "jsonrpc": "2.0",
                        "method": "notifications/initialized"
                    }
                    process.stdin.write(json.dumps(init_notification) + "\n")
                    process.stdin.flush()
                    
                    # 4. ì—°ê²° ì •ë³´ ì €ì¥
                    self.connections[config.name] = process
                    self.servers[config.name] = config
                    
                    print(f"âœ… MCP ì„œë²„ '{config.name}' ì—°ê²° ì„±ê³µ")
                    
                except asyncio.TimeoutError:
                    print(f"âŒ MCP ì„œë²„ '{config.name}' ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ")
                    return False
                except Exception as init_error:
                    print(f"âŒ MCP ì„œë²„ '{config.name}' ì´ˆê¸°í™” ì‹¤íŒ¨: {init_error}")
                    return False
                
                # ì„œë²„ì˜ ë„êµ¬ì™€ ë¦¬ì†ŒìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                await self._fetch_server_capabilities(config.name)
                
                print(f"âœ… MCP ì„œë²„ '{config.name}' ì—°ê²° ì„±ê³µ")
                return True
                
            else:
                # SSE, HTTP ì—°ê²°ì€ ì¶”í›„ êµ¬í˜„
                print(f"âŒ {config.type.value} íƒ€ì…ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ")
                return False
                
        except Exception as e:
            print(f"âŒ MCP ì„œë²„ '{config.name}' ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    async def _fetch_server_capabilities(self, server_name: str):
        """ì„œë²„ì˜ ë„êµ¬ì™€ ë¦¬ì†ŒìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        process = self.connections[server_name]
        
        # ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        try:
            # JSON-RPC ìš”ì²­ìœ¼ë¡œ ì§ì ‘ í†µì‹ 
            tools_request = {
                "jsonrpc": "2.0",
                "id": 2,
                "method": "tools/list"
            }
            
            process.stdin.write(json.dumps(tools_request) + "\n")
            process.stdin.flush()
            
            response_line = await asyncio.wait_for(
                asyncio.to_thread(process.stdout.readline), 
                timeout=5
            )
            tools_response = json.loads(response_line.strip())
            
            if "error" not in tools_response:
                tools = tools_response["result"]["tools"]
                self.tools[server_name] = [
                    McpTool(
                        name=tool["name"],
                        description=tool.get("description", ""),
                        input_schema=tool.get("inputSchema", {}),
                        server_name=server_name
                    )
                    for tool in tools
                ]
                print(f"ğŸ“š ì„œë²„ '{server_name}' ë„êµ¬ {len(tools)}ê°œ ë°œê²¬")
            else:
                print(f"âš ï¸  ì„œë²„ '{server_name}' ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {tools_response['error']}")
                self.tools[server_name] = []
        except Exception as e:
            print(f"âš ï¸  ì„œë²„ '{server_name}' ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            self.tools[server_name] = []
        
        # ë¦¬ì†ŒìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ê°„ë‹¨í™”)
        self.resources[server_name] = []  # ì¼ë‹¨ ìŠ¤í‚µ
    
    async def call_tool(self, server_name: str, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """MCP ë„êµ¬ ì‹¤í–‰"""
        if server_name not in self.connections:
            raise ValueError(f"MCP ì„œë²„ '{server_name}'ì´ ì—°ê²°ë˜ì§€ ì•ŠìŒ")
        
        process = self.connections[server_name]
        
        try:
            # JSON-RPC ë„êµ¬ ì‹¤í–‰ ìš”ì²­
            call_request = {
                "jsonrpc": "2.0",
                "id": 3,
                "method": "tools/call",
                "params": {
                    "name": tool_name,
                    "arguments": arguments
                }
            }
            
            process.stdin.write(json.dumps(call_request) + "\n")
            process.stdin.flush()
            
            response_line = await asyncio.wait_for(
                asyncio.to_thread(process.stdout.readline), 
                timeout=30
            )
            call_response = json.loads(response_line.strip())
            
            if "error" in call_response:
                return {
                    "success": False,
                    "content": f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {call_response['error']}",
                    "is_error": True
                }
            
            result = call_response["result"]
            
            # ê²°ê³¼ ì²˜ë¦¬
            content_parts = []
            for content in result.get("content", []):
                if content.get("type") == "text":
                    content_parts.append(content.get("text", ""))
            
            return {
                "success": True,
                "content": "\n".join(content_parts),
                "is_error": result.get("isError", False)
            }
            
        except Exception as e:
            return {
                "success": False,
                "content": f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
                "is_error": True
            }
    
    async def read_resource(self, server_name: str, uri: str) -> Dict[str, Any]:
        """MCP ë¦¬ì†ŒìŠ¤ ì½ê¸°"""
        if server_name not in self.connections:
            raise ValueError(f"MCP ì„œë²„ '{server_name}'ì´ ì—°ê²°ë˜ì§€ ì•ŠìŒ")
        
        session = self.connections[server_name]
        
        try:
            result = await session.read_resource(uri)
            
            content_parts = []
            for content in result.contents:
                if hasattr(content, 'text'):
                    content_parts.append(content.text)
            
            return {
                "success": True,
                "content": "\n".join(content_parts),
                "mime_type": getattr(result.contents[0], 'mimeType', None) if result.contents else None
            }
            
        except Exception as e:
            return {
                "success": False,
                "content": f"ë¦¬ì†ŒìŠ¤ ì½ê¸° ì˜¤ë¥˜: {str(e)}"
            }
    
    def get_all_tools(self) -> List[McpTool]:
        """ì—°ê²°ëœ ëª¨ë“  ì„œë²„ì˜ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
        all_tools = []
        for server_tools in self.tools.values():
            all_tools.extend(server_tools)
        return all_tools
    
    def get_server_info_for_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  MCP ì„œë²„ ì •ë³´ ìƒì„±"""
        if not self.connections:
            return "(No MCP servers currently connected)"
        
        server_descriptions = []
        for server_name, config in self.servers.items():
            if server_name in self.connections:
                tools_info = ""
                if server_name in self.tools and self.tools[server_name]:
                    tools_list = []
                    for tool in self.tools[server_name]:
                        schema_str = ""
                        if tool.input_schema:
                            schema_str = f"\n    Input Schema: {json.dumps(tool.input_schema, indent=2)}"
                        tools_list.append(f"- {tool.name}: {tool.description}{schema_str}")
                    tools_info = f"\n\n### Available Tools\n" + "\n\n".join(tools_list)
                
                cmd_info = f"{config.command}"
                if config.args:
                    cmd_info += f" {' '.join(config.args)}"
                
                server_descriptions.append(f"## {server_name} (`{cmd_info}`){tools_info}")
        
        return "\n\n".join(server_descriptions)
    
    async def cleanup(self):
        """ëª¨ë“  MCP ì—°ê²° ì •ë¦¬"""
        for server_name, process in self.connections.items():
            try:
                process.terminate()
                await asyncio.sleep(1)
                if process.poll() is None:
                    process.kill()
                process.wait()
                print(f"ğŸ§¹ MCP ì„œë²„ '{server_name}' ì—°ê²° ì •ë¦¬ë¨")
            except Exception as e:
                print(f"âš ï¸  ì„œë²„ '{server_name}' ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        self.connections.clear()
        self.servers.clear()
        self.tools.clear()
        self.resources.clear()

# ============================================================================
# 3. LLM API í´ë¼ì´ì–¸íŠ¸ - HTTP POST ë°©ì‹
# ============================================================================

class LLMClient:
    """HTTP POST ë°©ì‹ìœ¼ë¡œ LLM API í˜¸ì¶œí•˜ëŠ” í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.client = httpx.AsyncClient(timeout=60.0)
    
    async def generate_response(self, messages: List[LLMMessage]) -> str:
        """LLMì—ê²Œ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê³  ì‘ë‹µ ë°›ê¸°"""
        if self.config.provider == "openai":
            return await self._call_openai_api(messages)
        elif self.config.provider == "anthropic":
            return await self._call_anthropic_api(messages)
        elif self.config.provider == "ollama":
            return await self._call_ollama_api(messages)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {self.config.provider}")
    
    async def _call_openai_api(self, messages: List[LLMMessage]) -> str:
        """OpenAI API í˜¸ì¶œ"""
        url = self.config.base_url or "https://api.openai.com/v1/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.config.model,
            "messages": [{"role": msg.role, "content": msg.content} for msg in messages],
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens
        }
        
        response = await self.client.post(url, headers=headers, json=payload)
        response.raise_for_status()
        
        result = response.json()
        return result["choices"][0]["message"]["content"]
    
    async def _call_anthropic_api(self, messages: List[LLMMessage]) -> str:
        """Anthropic Claude API í˜¸ì¶œ"""
        url = self.config.base_url or "https://api.anthropic.com/v1/messages"
        
        headers = {
            "x-api-key": self.config.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì¼ë°˜ ë©”ì‹œì§€ ë¶„ë¦¬
        system_message = ""
        conversation_messages = []
        
        for msg in messages:
            if msg.role == "system":
                system_message = msg.content
            else:
                conversation_messages.append({"role": msg.role, "content": msg.content})
        
        payload = {
            "model": self.config.model,
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature,
            "messages": conversation_messages
        }
        
        if system_message:
            payload["system"] = system_message
        
        response = await self.client.post(url, headers=headers, json=payload)
        response.raise_for_status()
        
        result = response.json()
        return result["content"][0]["text"]
    
    async def _call_ollama_api(self, messages: List[LLMMessage]) -> str:
        """Ollama API í˜¸ì¶œ"""
        url = self.config.base_url or "http://localhost:11434/api/chat"
        
        payload = {
            "model": self.config.model,
            "messages": [{"role": msg.role, "content": msg.content} for msg in messages],
            "stream": False,
            "options": {
                "temperature": self.config.temperature,
                "num_predict": self.config.max_tokens
            }
        }
        
        response = await self.client.post(url, json=payload)
        response.raise_for_status()
        
        result = response.json()
        return result["message"]["content"]

# ============================================================================
# 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
# ============================================================================

class SystemPromptGenerator:
    """MCP ì •ë³´ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    def __init__(self, mcp_hub: McpHub):
        self.mcp_hub = mcp_hub
    
    def generate_system_prompt(self, mode: str = "assistant") -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ê¸°ë³¸ ì—­í•  ì •ì˜
        role_definition = self._get_role_definition(mode)
        
        # ë„êµ¬ ì‚¬ìš© ì§€ì¹¨
        tool_usage_section = self._get_tool_usage_section()
        
        # MCP ë„êµ¬ ì„¤ëª…
        mcp_tools_section = self._get_mcp_tools_section()
        
        # ì—°ê²°ëœ MCP ì„œë²„ ì •ë³´
        mcp_servers_section = self._get_mcp_servers_section()
        
        return f"""{role_definition}

{tool_usage_section}

{mcp_tools_section}

{mcp_servers_section}

## ê·œì¹™
1. ê° ë„êµ¬ ì‚¬ìš© í›„ ë°˜ë“œì‹œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ì„¸ìš”.
2. MCP ë„êµ¬ ì‚¬ìš© ì‹œ ì •í™•í•œ ì„œë²„ ì´ë¦„ê³¼ ë„êµ¬ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
3. ë„êµ¬ ë§¤ê°œë³€ìˆ˜ëŠ” JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš”.
4. ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
"""
    
    def _get_role_definition(self, mode: str) -> str:
        """ì—­í•  ì •ì˜"""
        if mode == "assistant":
            return """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
        elif mode == "code":
            return """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì½”ë“œ ì‘ì„±, ë¶„ì„, ë””ë²„ê¹…ì„ ë„ì™€ì¤ë‹ˆë‹¤."""
        else:
            return """ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."""
    
    def _get_tool_usage_section(self) -> str:
        """ë„êµ¬ ì‚¬ìš© ì§€ì¹¨"""
        return """## ë„êµ¬ ì‚¬ìš©

ë‹¤ìŒ XML í˜•ì‹ìœ¼ë¡œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

<tool_name>
<parameter1>value1</parameter1>
<parameter2>value2</parameter2>
</tool_name>

ì˜ˆì‹œ:
<use_mcp_tool>
<server_name>weather-server</server_name>
<tool_name>get_weather</tool_name>
<arguments>{"city": "Seoul", "units": "celsius"}</arguments>
</use_mcp_tool>"""
    
    def _get_mcp_tools_section(self) -> str:
        """MCP ë„êµ¬ ì„¤ëª…"""
        return """## ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬

### use_mcp_tool
MCP ì„œë²„ì˜ ë„êµ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
ë§¤ê°œë³€ìˆ˜:
- server_name: (í•„ìˆ˜) MCP ì„œë²„ ì´ë¦„
- tool_name: (í•„ìˆ˜) ì‹¤í–‰í•  ë„êµ¬ ì´ë¦„  
- arguments: (í•„ìˆ˜) ë„êµ¬ì— ì „ë‹¬í•  ë§¤ê°œë³€ìˆ˜ (JSON í˜•ì‹)

### access_mcp_resource
MCP ì„œë²„ì˜ ë¦¬ì†ŒìŠ¤ì— ì ‘ê·¼í•©ë‹ˆë‹¤.
ë§¤ê°œë³€ìˆ˜:
- server_name: (í•„ìˆ˜) MCP ì„œë²„ ì´ë¦„
- uri: (í•„ìˆ˜) ì ‘ê·¼í•  ë¦¬ì†ŒìŠ¤ URI"""
    
    def _get_mcp_servers_section(self) -> str:
        """ì—°ê²°ëœ MCP ì„œë²„ ì •ë³´"""
        server_info = self.mcp_hub.get_server_info_for_prompt()
        
        return f"""## ì—°ê²°ëœ MCP ì„œë²„

{server_info}

ì„œë²„ê°€ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©´ `use_mcp_tool` ë„êµ¬ë¡œ ì„œë²„ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê³ , `access_mcp_resource` ë„êµ¬ë¡œ ë¦¬ì†ŒìŠ¤ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

# ============================================================================
# 5. ë„êµ¬ ì‹¤í–‰ ì²˜ë¦¬ê¸°
# ============================================================================

class ToolExecutor:
    """LLM ì‘ë‹µì—ì„œ ë„êµ¬ í˜¸ì¶œì„ íŒŒì‹±í•˜ê³  ì‹¤í–‰"""
    
    def __init__(self, mcp_hub: McpHub):
        self.mcp_hub = mcp_hub
    
    async def parse_and_execute_tools(self, llm_response: str) -> List[Dict[str, Any]]:
        """LLM ì‘ë‹µì—ì„œ ë„êµ¬ í˜¸ì¶œì„ íŒŒì‹±í•˜ê³  ì‹¤í–‰"""
        import re
        
        results = []
        
        # use_mcp_tool íŒ¨í„´ ì°¾ê¸°
        mcp_tool_pattern = r'<use_mcp_tool>\s*<server_name>([^<]+)</server_name>\s*<tool_name>([^<]+)</tool_name>\s*<arguments>([^<]+)</arguments>\s*</use_mcp_tool>'
        
        for match in re.finditer(mcp_tool_pattern, llm_response, re.DOTALL):
            server_name = match.group(1).strip()
            tool_name = match.group(2).strip()
            arguments_str = match.group(3).strip()
            
            try:
                arguments = json.loads(arguments_str)
                result = await self.mcp_hub.call_tool(server_name, tool_name, arguments)
                
                results.append({
                    "type": "use_mcp_tool",
                    "server_name": server_name,
                    "tool_name": tool_name,
                    "arguments": arguments,
                    "result": result
                })
                
            except Exception as e:
                results.append({
                    "type": "use_mcp_tool",
                    "server_name": server_name,
                    "tool_name": tool_name,
                    "error": str(e)
                })
        
        # access_mcp_resource íŒ¨í„´ ì°¾ê¸°
        mcp_resource_pattern = r'<access_mcp_resource>\s*<server_name>([^<]+)</server_name>\s*<uri>([^<]+)</uri>\s*</access_mcp_resource>'
        
        for match in re.finditer(mcp_resource_pattern, llm_response, re.DOTALL):
            server_name = match.group(1).strip()
            uri = match.group(2).strip()
            
            try:
                result = await self.mcp_hub.read_resource(server_name, uri)
                
                results.append({
                    "type": "access_mcp_resource",
                    "server_name": server_name,
                    "uri": uri,
                    "result": result
                })
                
            except Exception as e:
                results.append({
                    "type": "access_mcp_resource",
                    "server_name": server_name,
                    "uri": uri,
                    "error": str(e)
                })
        
        return results

# ============================================================================
# 6. ë©”ì¸ ëŒ€í™” ê´€ë¦¬ì
# ============================================================================

class ConversationManager:
    """ì „ì²´ ëŒ€í™”ë¥¼ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, llm_config: LLMConfig):
        self.mcp_hub = McpHub()
        self.llm_client = LLMClient(llm_config)
        self.prompt_generator = SystemPromptGenerator(self.mcp_hub)
        self.tool_executor = ToolExecutor(self.mcp_hub)
        self.conversation_history: List[LLMMessage] = []
    
    async def setup_mcp_servers(self, server_configs: List[McpServerConfig]):
        """MCP ì„œë²„ë“¤ ì—°ê²° ì„¤ì •"""
        print("ğŸ”§ MCP ì„œë²„ ì—°ê²° ì¤‘...")
        for config in server_configs:
            await self.mcp_hub.connect_server(config)
        
        print(f"âœ… ì´ {len(self.mcp_hub.connections)}ê°œ MCP ì„œë²„ ì—°ê²°ë¨")
        
        # ì—°ê²°ëœ ë„êµ¬ë“¤ ì¶œë ¥
        all_tools = self.mcp_hub.get_all_tools()
        if all_tools:
            print(f"ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {', '.join([tool.name for tool in all_tools])}")
    
    async def process_user_message(self, user_message: str) -> str:
        """ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬"""
        print(f"ğŸ‘¤ ì‚¬ìš©ì: {user_message}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.conversation_history.append(LLMMessage(role="user", content=user_message))
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = self.prompt_generator.generate_system_prompt()
        
        # LLM API í˜¸ì¶œ
        messages = [LLMMessage(role="system", content=system_prompt)] + self.conversation_history
        
        llm_response = await self.llm_client.generate_response(messages)
        print(f"ğŸ¤– LLM ì‘ë‹µ: {llm_response}")
        
        # ë„êµ¬ í˜¸ì¶œ íŒŒì‹± ë° ì‹¤í–‰
        tool_results = await self.tool_executor.parse_and_execute_tools(llm_response)
        
        final_response = llm_response
        
        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
        if tool_results:
            print(f"ğŸ”§ {len(tool_results)}ê°œ ë„êµ¬ ì‹¤í–‰ë¨")
            
            # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ëŒ€í™”ì— ì¶”ê°€
            self.conversation_history.append(LLMMessage(role="assistant", content=llm_response))
            
            tool_results_text = ""
            for result in tool_results:
                if "error" in result:
                    tool_results_text += f"âŒ ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {result['error']}\n"
                else:
                    tool_results_text += f"âœ… ë„êµ¬ ì‹¤í–‰ ê²°ê³¼:\n{result['result']['content']}\n\n"
            
            # ë„êµ¬ ê²°ê³¼ë¥¼ í¬í•¨í•´ì„œ ë‹¤ì‹œ LLM í˜¸ì¶œ
            if tool_results_text.strip():
                self.conversation_history.append(LLMMessage(role="user", content=f"ë„êµ¬ ì‹¤í–‰ ê²°ê³¼:\n{tool_results_text}"))
                
                messages = [LLMMessage(role="system", content=system_prompt)] + self.conversation_history
                final_response = await self.llm_client.generate_response(messages)
                print(f"ğŸ¤– ìµœì¢… ì‘ë‹µ: {final_response}")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ìµœì¢… ì‘ë‹µ ì¶”ê°€
        self.conversation_history.append(LLMMessage(role="assistant", content=final_response))
        
        return final_response

# ============================================================================
# 7. ì‚¬ìš© ì˜ˆì œ
# ============================================================================

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # LLM ì„¤ì •
    llm_config = LLMConfig(
        provider="ollama",  # "openai", "anthropic", "ollama" ì¤‘ ì„ íƒ
        model="llama3.2",
        base_url="http://localhost:11434",  # Ollama ê¸°ë³¸ URL
        temperature=0.7
    )
    
    # ëŒ€í™” ê´€ë¦¬ì ì´ˆê¸°í™”
    manager = ConversationManager(llm_config)
            # command="a:\\simpledraw-mcp\\venv\\Scripts\\python.exe",
            # args=["a:\\simpledraw-mcp\\simpledraw.py"],  # ì‹¤ì œ MCP ì„œë²„ ê²½ë¡œ
    # MCP ì„œë²„ ì„¤ì • (ì˜ˆ: ë‚ ì”¨ ì„œë²„)
    mcp_servers = [
        McpServerConfig(
            name="example-server",
            type=McpServerType.STDIO,
            command="a:\\mcpclient\\venv\\Scripts\\python.exe",
            args=["a:\\mcpclient\\example_mcp_server.py"],  # ê°„ë‹¨í•œ MCP ì„œë²„
            timeout=10  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
        ),
        # ì¶”ê°€ MCP ì„œë²„ë“¤...
    ]
    
    # MCP ì„œë²„ ì—°ê²°
    await manager.setup_mcp_servers(mcp_servers)
    
    # ëŒ€í™” ì‹œì‘
    print("\nğŸ’¬ MCP + LLM ëŒ€í™” ì‹œì‘! (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
    print("=" * 50)
    
    while True:
        user_input = input("\nğŸ‘¤ ë‹¹ì‹ : ")
        
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        try:
            response = await manager.process_user_message(user_input)
            print(f"\nğŸ¤– AI: {response}")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # ì •ë¦¬
    await manager.mcp_hub.cleanup()
    await manager.llm_client.client.aclose()

# ============================================================================
# 8. Streamlit ì±—ë´‡ ì˜ˆì œ (ì£¼ì„)
# ============================================================================

"""
Streamlit ì±—ë´‡ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì˜ˆì œ:

```python
import streamlit as st
import asyncio

# ì „ì—­ ë³€ìˆ˜ë¡œ ConversationManager ì €ì¥
if 'conversation_manager' not in st.session_state:
    st.session_state.conversation_manager = None
    st.session_state.setup_complete = False

def load_mcp_config(config_file: str = "test.json") -> List[McpServerConfig]:
    \"\"\"JSON ì„¤ì • íŒŒì¼ì—ì„œ MCP ì„œë²„ ì„¤ì • ë¡œë“œ\"\"\"
    import json
    import os
    
    if not os.path.exists(config_file):
        st.error(f"MCP ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file}")
        return []
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        mcp_servers = []
        for server_name, server_config in config_data.get("mcpServers", {}).items():
            mcp_servers.append(McpServerConfig(
                name=server_name,
                type=McpServerType.STDIO,
                command=server_config.get("command"),
                args=server_config.get("args", []),
                env=server_config.get("env"),
                timeout=server_config.get("timeout", 30)
            ))
        
        return mcp_servers
        
    except Exception as e:
        st.error(f"MCP ì„¤ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return []

async def setup_mcp_system():
    \"\"\"MCP ì‹œìŠ¤í…œ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰)\"\"\"
    if st.session_state.setup_complete:
        return st.session_state.conversation_manager
    
    # LLM ì„¤ì •
    llm_config = LLMConfig(
        provider="ollama",
        model="llama3.2", 
        base_url="http://localhost:11434",
        temperature=0.7
    )
    
    # ëŒ€í™” ê´€ë¦¬ì ì´ˆê¸°í™”
    manager = ConversationManager(llm_config)
    
    # JSON íŒŒì¼ì—ì„œ MCP ì„œë²„ ì„¤ì • ë¡œë“œ
    mcp_servers = load_mcp_config("test.json")  # test.json íŒŒì¼ì—ì„œ ì„¤ì • ì½ê¸°
    
    if not mcp_servers:
        st.warning("ì—°ê²°í•  MCP ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤. test.json íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    
    # MCP ì„œë²„ ì—°ê²°
    await manager.setup_mcp_servers(mcp_servers)
    
    st.session_state.conversation_manager = manager
    st.session_state.setup_complete = True
    return manager

async def chat_with_mcp(user_message: str) -> str:
    \"\"\"MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬\"\"\"
    manager = await setup_mcp_system()
    
    try:
        response = await manager.process_user_message(user_message)
        return response
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def main_streamlit():
    \"\"\"Streamlit ë©”ì¸ í•¨ìˆ˜\"\"\"
    st.title("ğŸ¤– MCP + LLM ì±—ë´‡")
    st.caption("MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸")
    
    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                # asyncio.runì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
                response = asyncio.run(chat_with_mcp(prompt))
                st.markdown(response)
        
        # AI ì‘ë‹µ ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": response})

# Streamlit ì‹¤í–‰
if __name__ == "__main__":
    # ì¼ë°˜ ì½˜ì†” ëª¨ë“œ
    asyncio.run(main())
    
    # Streamlit ëª¨ë“œë¡œ ì‹¤í–‰í•˜ë ¤ë©´:
    # streamlit run python_mcp_llm_example.py
```

ì‚¬ìš©ë²•:
1. í„°ë¯¸ë„ì—ì„œ: `streamlit run python_mcp_llm_example.py`
2. ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì—´ë¦¬ëŠ” í˜ì´ì§€ì—ì„œ ì±„íŒ…
3. MCP ë„êµ¬ë“¤ì´ ìë™ìœ¼ë¡œ ì‚¬ìš©ë¨ (ì‹œê°„ ì¡°íšŒ, ê³„ì‚°, í…ìŠ¤íŠ¸ ë¶„ì„, UUID ìƒì„± ë“±)

ì˜ˆì œ ì…ë ¥:
- "í˜„ì¬ ì‹œê°„ ì•Œë ¤ì¤˜"
- "2 + 3 * 4ë¥¼ ê³„ì‚°í•´ì¤˜" 
- "ì•ˆë…•í•˜ì„¸ìš” í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
- "UUID í•˜ë‚˜ ìƒì„±í•´ì¤˜"
"""

if __name__ == "__main__":
    asyncio.run(main()) 